tf-idf -> term frequency-inverse term frequency

from nltk.text import Text
text1 = Text(nltk.corpus.gutenberg.words("file_path"))
--> Convert file to nltk text object

text1.concordance("word") --> find sentence in text1 where "word" in present
text1.similarity("word") --> find words that have similar context as "word"
text1.common_contexts(["word1","word2"]) --> find contexts that are shared by "word1" and "word2"
text1.dispresion_plot(["word1","word2","word3"]) --> draw dispresion plot of "word1","word2" and "word3" in text1
text1.collocations() --> output all collocation of 2 words(bigrams common in text1)


fdist = FreqDist({text/number list/string}) --> Find frequency distribution of the input
  -fdist.keys() --> output Keys of the distribution in order of decreasing frequency
  -fdist.items() --> tuple of key value of frequency distribution
  -fdist.inc(sample) --> Increment the count for this sample
  -fdist['monstrous'] --> Count of the number of times a given sample occurred
  -fdist.freq('monstrous') --> Frequency of a given sample
  -fdist.N() --> Total number of samples
  -for sample in fdist: --> Iterate over the samples, in order of decreasing frequency
  -fdist.max() --> Sample with the greatest count
  -fdist.tabulate() --> Tabulate the frequency distribution
  -fdist.plot() --> Graphical plot of the frequency distribution
  -fdist.plot(cumulative=True) --> Cumulative plot of the frequency distribution


POS-Tags
CC - coordinating conjunction
RB - adverbs
IN - preposition
NN - common noun
NNP - proper noun
NNPS - proper noun plural
NNS - common noun plural
JJ - adjective

nltk --
nltk.help.upenn_tagset('NN.*') -- use regex to get help for tags
nltk.help.upenn_tagset('RB') -- get help for tag 'RB'


nltk.tokenize.word_tokenize(text) --> tokenize each word in the text
nltk.tokenize.sent_tokenize(text) --> tokenize each sentence in the text
nltk.tokenize.regexp_tokenize(text, regexp) --> tokenize text using regexp
nltk.pos_tag(tokenized_text) --> give part of speech tag to each token
nltk.ne_chunk(tokenized_pos'ed_text) --> give NER tagged trees to text

collections.Counter(tokenized_text) --> ouputs adictionary with count of each word in the text
nltk.corpus.stopwords --> list of stop words for all languages

nltk.stem.WordNetLemmatizer --> used for lemmatization



gensim --
gensim.corpora.dictionary.Dictionary(tokenized_text) --> give id to each token
dictionary.doc2bow(doc) --> covert to bag of words model uning token id
gensim.models.tfidfmodel.TfidfModel(gensim_bow) --> generate tf-idf model


Spacy --
nlp = spacy.load('en')
doc = nlp(text_doc)
doc.ents --> ouputs entities
doc.ents[0].label_ --> ouputs label of the entity

Ployglot -- spacy for 130 diff languages
