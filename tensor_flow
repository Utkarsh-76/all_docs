There are two parths of a tensor flow program
  1. Create computation graph
  2. Run the graph
  
Three types of declarations:
  - tf.placeholder(tf.float32, shape=())  --> to define a placeholder that will be used to hold a value(in feed_dict) when 
                                    we run the session(like feature and label values in a neural network)
  - tf.constant(5)              --> constant value 5
  - tf.Variable()               --> like weights and bias in a neural network
  
  
after declaring the placeholders, define your model.

optimizer = tf.train.AdamOptimizer{or other optimizer}(learning_rate = ...).minimize(...)
init = tf.global_variables_initializer()
for tf.Session as sess:
  tf.run(init)
  for epoc in range(epocs_num):
    _, cost = sess.run([...{optimizer}, ...{cost}], feed_dict = {X:..., Y:...})
    
parameters = sess.run(parameters) --> save best model weights and biases in parameters
  
  
tf.sigmoid(x) --> sigmod funtion on x
tf.nn.relu(x) --> relu funtion on x

tf.nn.sigmoid_cross_entropy_with_logits(logits = ...,  labels = ...) --> cost funtion calculation using sigmoid activation 
                                                                          in last layer
tf.nn.softmax_cross_entropy_with_logits(logits = ..., labels = ...) --> cost funtion calculation using softmax activation 
                                                                          in last layer
tf.reduce_mean(X) --> summation over examples
tf.matmul(A,B)  --> matrix multiplication of A and B
tf.one_hot(labels,depth,axis) --> convert labels to one hot encoding
tf.get_variable(...{name} , ...{shape}, ...{dtype} , initializer = tf.contrib.layers.xavier_initializer/tf.zero_initializer)
          --> weight initialization
tf.equal() --> compare two tensors


CNN
tf.nn.conv2d(X,W1, strides = [1,s,s,1], padding = 'SAME'): 
    given an input X and a group of filters W1, this function convolves W1's filters on X. 
    The third input ([1,s,s,1]) represents the strides for each dimension of the input (m, n_H_prev, n_W_prev, n_C_prev)

tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = 'SAME'): 
    given an input A, this function uses a window of size (f, f) and 
    strides of size (s, s) to carry out max pooling over each window. 

tf.nn.relu(Z1): 
    computes the elementwise ReLU of Z1 (which can be any shape).
    
tf.contrib.layers.flatten(P): 
    given an input P, this function flattens each example into a 1D vector it while maintaining the batch-size. 
    It returns a flattened tensor with shape [batch_size, k]

tf.contrib.layers.fully_connected(F, num_outputs): 
    given a flattened input F, it returns the output computed using a fully connected layer.
    
tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y): 
    computes the softmax entropy loss. This function both computes the softmax activation 
    function as well as the resulting loss. 
