Cheat sheets for ML algos
https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463

##Intalling a package
1. Download the package
2. Open terminal
3. Go to the directory where the file is placed and unzip it
4. Run "python setup.py install"

##Debugging
import pdb; pdb.set_trace()

importing a function from a package
from functools import reduce

##Syntax
List --> l = [a,b,d,e,c] (list can contain different datatype)
calling List --> l[0] gives element a, l[-1] gives element c
slicing list --> l[2:4] gives d,e
concat/adding list --> l + [f,g] => [a,b,d,e,c,f,g]
delete element from list --> del(l[2]) => [a,b,e,c]
get index of element --> l.index("c") => 4
adding element in the end --> l.append('k')

length of string/list --> len(str)
calling substring --> str[2:7]
splitting string into list--> str_split = str.split('_'){default splits with ' '}
get 1st splitted string --> str_split.str.get(0)
string contains a regex --> str.contains()
string replace --> str.replace(old, new[, max number of replacements])
count repetition of substr --> str.count(substr)
s.find(t) --> Index of first instance of string t inside s (-1 if not found)
s.rfind(t) --> Index of last instance of string t inside s (-1 if not found)
s.index(t) --> Like s.find(t), except it raises ValueError if not found
s.rindex(t) --> Like s.rfind(t), except it raises ValueError if not found
s.join(text) --> Combine the words of the text into a string using s as the glue
s.splitlines() --> Split s into a list of strings, one per line
s.lower() --> A lowercased version of the string s
s.upper() --> An uppercased version of the string s
s.title() --> A titlecased version of the string s
s.strip() --> A copy of s without leading or trailing whitespace
any(list) --> true if any lement in the list is true

numpy array --> np_arr = np.array(["a","b","c"])
size of numpy array --> np_arr.shape
calling element from numpy array --> np_arr[0,2] gives "c"
np.logical_and(),np.logical_or(),np.logical_not() are used as logical operators on numpy arrays
looping over 2D np.array
for val in np.nditer(np_2d_arr)

plotting
making a plot --> plt.plot(x = x_axis,y = y_axis) {two dimensional}     plt.plot(y_axis) {one dimensional--set integer on x-axis}
showing plot on screen --> plt.show()
line graph --> df.plot(subplots=True{plot all columns of df seperately})
scatter plot --> plt.scatter(x_vals,y_vals) / df.plot(kind='scatter', x='1800',y='1900',s=radius_in_pixels_for_each_point)
histogram --> plt.hist(vals,bin_nos) / df.plot(kind='hist')
boxplot --> plt.boxplot(columns='data_column',by='group_by_column_name')
plt.xlabel(x_lab)
plt.ylabel(y_lab)
plt.title(title)
plt.savefig('fig.png/fig.jpg/fig.pdf')
plt.subplot(nrows=3,ncols=1) --> create 3 subplots as 3*1 matrix
df.plot(style='color_marker_lineType')
bar plots --> https://plot.ly/matplotlib/bar-charts/

Dictionaries
making dictionaries --> dict = {"a":1,"b":2,"c":3} , dict(list_of_tuple)
calling value in dict --> dict["b"] => 2
delete a key-value pair --> del(dict["c"]) => {"a":1,"b":2}
looping over dictionary
for key,value in dict.items()
adding element in dictonary --> langs_count["d"]=4

DataFrame
making dataframe --> df = pd.DataFrame(dictionary_name)
empty dataframe   --> df = pd.DataFrame()
setting label --> df.label = label_list
DataFrame from a csv --> pd.read_csv('names.csv' , index_col = 0{specifyingby index column}, header = None/0/1, 
                          names{specify column names}, na_vlaues{string/number that will be replaced by NaN}.
                          parse_dates{covert multiple date(year,month,date) column to one column}, nrows=5{no of rows to parse}
                          sep= '\t'{data separator}, comment='#'{cosider line after # as comment and ignore})
              for chunk in pd.read_csv('data.csv', chunksize=1000{specifying the size of chunk to take at once})
DfataFrame to csv --> df.to_csv('my_data.csv', index = False{do not save index in file})
DfataFrame to excel --> df.to_excel('my_data.xlsx')
selecting a column --> brics['country']/brics.country  (this returns a pandas series with labels)
                   --> brics[['country']] (this returns a pandas DataFrame)
                   --> brics[:,0:3](return 1st to 3rd column)
selecting a rows   --> brics[1:4] outputs rows 2,3 and 4
                   --> brics.loc['RU'] => row with 'RU' label (type is a pd series)
                   --> brics.loc[['RU']] => row with 'RU' label (type is a pd DataFrame)
selecting specific rows and cols  --> brics.loc[["RU","IN"],["country"]] => country col of rows 'RU' and 'IN'
use brics.loc[:,["country"]] to get full 'country column'
use iloc if you want to access data using positions
selecting rows using some condition --> brics[brice['area']>8]
looping over DataFrame
for label,row in df.iterrows()    =>  label contains label of hte row, row contains data of the row as pandas series
brics["a"] = brics["b"].apply(len)  --> apply len function on "b" column of brics and save that to column "a"
appending to a dataframe  --> df.append(df2)
df.head() --> display first 5 values and header
df.tail() --> display last 5 values and header
df.vaues  --> covert to numpy array
read excel file --> data = pd.ExcelFile(file)
                    data.sheet_names  --> gives name of sheets in the excel file
                    data.parse(sheetname/sheet_index, skip_rows=[0,1], names=['Country','State'], 
                    parse_cols=[0]) --> parse the specified sheet in dataframe
                    
df.columns  --> return column names
df.shape    --> number of rows and cols of DataFrame
df.info()   --> get info of the dataframe
df.continent.value_counts(dropna=False) //  frequency of each data in the column=continent
df.describe() //  describe the numeric fields of the dataframe --> give out count,mean,median etc.
df.isnull().sum() // get number of nan values in each column
df.count() --> count of non-null values of each col in dataframe
df.value_count() --> count frequency of each value
df.mean() --> mean of each col in dataframe ignoring null entries
df.std() --> standard deviation of each col in dataframe ignoring null entries
df.median()
df.min()
df.max()
df.quantile(0.5/0.6) --> observation having 50/60 percent data smaller than itself
df.boxplot(column='population',by='continent')
df.type() --> get details about the dataframe
df.apply(funtion_to_apply_to_df, axis = 0/1{for each column/row}, pattern=pat{pass additional parameter to the function})
df.map(funtion to apply to index)-->map function to index of df
df[col_name].map(dictionary to apply to col)
df.init_cost.replace("$"," ") {replace $ with ' ' in whole column}
df.drop_duplicates --> drop duplicate rows from df
df.dropna() --> drop rows with NaN values in them
pd.concat([df1,df2], ignore_index=True{ignore original index}, axis=1{1 for horizontal concat})
pd.merge(left = df_l, right = df_r, on={name of col to join with}, left_on={name of col in df_l}, right_on={name of col in df_r})
df.reindex(list_of_new_df_index, method = 'ffill/bfill'{fill non-matching entries with preceding/succeding non-null entry})
df.resample('D').mean() --> resample data to daily samples and display mean of each sample{must have datetime as the index}
df.resample('D').mean().interpolate('linear') --> linear interpolation for missing data
df.rolling(window=24).mean --> display mean of each point with 24hrs window stretching out from that point
df.drop(col_list_to_drop)
df.all() --> Whick cols have all non-zero values
df.any() --> Which cols have any non-zero values
df.floordiv(n) --> diide all element in df by 'n' and take floor

for loop
for height in fam:
  print(fam)          =>  this prints all height in fam list

for index,height in enumerate(fam):   =>    gives index as well as height from fam list

Random number generator --> np.random.rand()
----- "" ---- seed  --> np.random.seed(123)
Random integer generator --> np.random.randint(0,7) => generate random integer from 0 to 6

1 to 100 series --> range(100)

function defination:-
def square(orig_no,pow = 1{default argument}):
  """Returns squre of a value"""      --> docstrings
  new_val = orig_val ** pow
  return new_val
return a tuple if you want to return more than 1 result
pass multiple arguments --> *args
pass multiple arguments with arguments --> **kwargs
  
tuple
t = (2,3,4)
a,b,c = t   -> a => 2, b=>3, c=>4
accessing an element  -->   t[1] => 3

Lamda functions syntax
rtp = lamda x, y: x**y
function without name is anonymous function(used to pass a function into another function)

map function applies a function(arg 1) to all elements of arg 2 
filter function filters arg 2 with arg 1 condition(result = filter(lambda x:len(x)>6, fellowship))
reduce function applies function arg 1 in a series to all elements of args 2

## Exceptions
use try, except blocks to catch exceptions
TypeError to check for type exceptions
Raise ValueError to specify unacceptable values

##Iterators
convert iterable into iterator --> it = iter(iterable)
use next(it) to go to next element of iterator
enumerate(avengers,start=10{to specify start index}) --> takes a iterable as argument and add index to it
zip(avengers,names) --> zip more that 1 iterabels together

##List comprehensions
new_num = [n + 1 for n in num]
num_pair = [(num1,num2) for num1 in range(2,4) for num2 in (4,6)]
even_num = [num for num in range(10) if num % 2 == 0]
even_num = [num if num % 2 == 0 else 0 for num in range(10)]
pos_neg = {num: -num for num in range(6)}   -->   Creating a dictionary

##generator
replace [] with () in list comprehensions to make a generator.
Generator is an iterator and behaves exactly like an iterator.
generator funtion   -->   instead of return, use yield in the function

##file generators
read 1 line from the file generator -->   file.readline()

##Reading data from files
Text file --> file = open(filename,mode='r'{for read only})
              text = file.read()  --> reading the text of the file
              file.close()  --> closing the file
              
              with open(filename,mode='r') as file:
              print(file.read())
              
              file.readline() --> read one line from the file
              file.readlines() --> read multiple lines
              
              for line in file/open(filename)
              
using numpy --> np.loadtxt(filename,delimiter=',',skiprows=1,usecols=[0,2])
                data = np.genfromtxt('titanic.csv', delimiter=',', names=True, dtype=None) --> {load different type of data}

##Import SAS files
from sastbdat import SAS7BDAT
  with SAS7BDAT(fname) as file:
    df_sas = file.to_data_frame()
    
##Import stata files
pd.read_stata(fname)

##Import HDF5 file
import h5py
data = h5py.file(fname,'r')

##Import mat file
import scipy.io
data = scipy.io.loadmat(fname)

##Import from DataBase
  from sqlalchemy import create_engine
  engine = create_engine('sqlite:///Northwind.sqlite')
  con = engine.connect()
  rs = con.execute("select * from Orders")
  df = pd.DataFrame(rs.fetchall())
  df.columns = rs.keys()
  con.close()
  /
  with engine.connect() as con
    rs = con.execute("select * from Orders")
    df = pd.DataFrame(rs.fetchmany(size=5))
    df.coumns = rs.keys()
  df = pd.read_sql_query("select * from orders", engine)
  
##Import data from www
  from urllib.request import urlretrieve
  urlretrieve(csv_file_url,csv_file_name) // save file locally
  df = pd.read_csv(url,sep) // read csv data directly into dataframe

##Import HTML data
  import requests
  r = requests.get(url)
  text = r.text // get HTML text in string
  json = r.json() //  if the incoming data is in json format
  /
  from urllib.request import Request,urlopen
  request=Request(url)
  response = urlopen(request)
  html = response.read()

##Beautiful Soup
from bs4 import BeautifulSoup
soup = BeautifulSoup(html_text) // convert html text into BeautifulSoup object
pretty_soup = soup.prettify()  //  make the BeautifulSoup look pretty(proper indentation of tags and all)
soup.title  //  to get title of the html page
soup.get_text() //  to get the text of html page
a_tags = soup.find_all('a')  //  find all a tags
for link in a_tags:
    print(link.get('href')) //  get attribute of a tag

##Reading json
import json
with open(json_fname,'r') as json_file
  json_data = json.load(json_file)  //  json_data is a dictionary

pd.melt(frame = df, id_vars = 'name'{columns to keep constant}, value_vars=['trea a','trea b']{comlumn names to melt},
var_name='treatment'{variable col name}, value_name = 'result'{name of the value column})

df.pivot(index='date'{column to be fixed}, columns='element'{column to pivot}, values='values'{values to fill in the df})
df.pivot_table(index='date', columns='element', values='values', aggfunc=np.mean) // to take care of duplicate values

##Globbing
import glob
csv_files = glob.glob(*.csv){get all the csv files}

##Data Cleaning
df['a']=df['a'].astype(str/'category'){convert numeric col to string/categorical data}
df['a']=pd.to_numeric(df['a'],errors='coerce'){convert to numeric column with non-numeric data coverted to NaN}

##Regular Expressions
https://docs.python.org/3/library/re.html
[abc$]      -- match a,b,c,$
[^ac]       -- match any character except a/c
\w          -- match an alpha-numeric character ~~ [a-zA-Z0-9_]
\w          -- match an non alpha-numeric character ~~ [^a-zA-Z0-9_]
\d          -- match a single digit
\D          ~~ [^0-9]
\s          -- match any whitespace character ~~ [ \t\n\r\f\v]
\S          -- match any non whitespace character ~~ [^ \t\n\r\f\v]
.           -- match any one character except a new line
\d*         -- match digits 0 or more times
\d+         -- match digits 1 or more times
\d?         -- match digits 0 or 1 time
\d{n}       -- match exactly n digits
\d{m,n}     -- match min m and max n digits
r"\n"       -- match \n exactly(r for raw string)
\$,\# etc.  -- escape character, use $,# etc. as it is
^           -- match the pattern at the beginning
$           -- match the pattern at the end 
^\d*$       -- this will match strings that have only digits{'1'} and not str --> 'I have 10 rupee'
+           -- match the previous one with 1 or more matches
[A-Z]       -- match a capital letter
[\w.-]+     -- match a word/./- any number of times(+)
A-Za-z      -- match capital/small letter
[A-Za-z\.\s]* -- any capital letter, any . and any ' ' is acceptable

p = re.compile('RE pattern')  --> compile the RegEx
m = p.match --> match 'REp' default starting at index 0
m = p.search --> match 'REp' default starting at any index
m.group() --> return the matching string
m.start() --> index of start of matching string
m.end() --> index of end of matching string
m.span() --> return (m.start(),m.end())
p.findall --> match all instace of 'REp' and return matching strings
all above functions can be used as re.findall/match/search/sub(pattern,str){skipping compile step}

import re
  pattern = re.compile('^\d*$')
  result = pattern.match('10')
  bool(result) --> gives True

##Missing Values
df[['a','b']] = df[['a','b']].fillna(0) --> fill missing values in a and b column with 0
df['a'] = df['a'].fillna('missing') --> fill missing vlaues with 'missing' string
df['a'] = df['a'].fillna(df['a'].mean()) --> fill missing values with mean of the column
https://www.analyticsindiamag.com/5-ways-handle-missing-values-machine-learning-datasets/

.all() --> return single T/F for all T/F result of a column

##Series
Ser.str.upper() --> covert the series to upper case characters
Ser.str.contains('sub_str') --> return T/F series 
pd.series(value_list, index = index_list)

##Multi-indexing
df.set_index(['a','b']) --> set a,b as mulit-index
df.sort_index --> sort the index with 1st index not repeating
df.loc[('index1_identifier', 'index2_identifier'),'column'] 
df.loc[(slice(None){used in tuple index instead of :}, slice('index2_indentifier1', 'index2_indentifier2')),:] 

##groupby
df.groupby(col_to_groupby).count() {give count for distinct values of col which we grouped by}
df.groupby(col_to_groupby)[col_to_operate_on].agg()
df.groupby(col_to_groupby).agg(aggfunc/user defined aggregation func/dictionary)

df.col.idxmax(){row-wise}/df.col.idxmax(axis='columns'){col-wise} --> give lable with max value
df.col.idxmin()
.nunique() --> gives number of unique values in a col

s.startswith(t) --> Test if s starts with t
s.endswith(t) --> Test if s ends with t
t in s --> Test if t is contained inside s
s.islower() --> Test if all cased characters in s are lowercase
s.isupper() --> Test if all cased characters in s are uppercase
s.isalpha() --> Test if all characters in s are alphabetic
s.isalnum() --> Test if all characters in s are alphanumeric
s.isdigit() --> Test if all characters in s are digits
s.istitle() --> Test if s is titlecased (all words in s have initial capitals)
