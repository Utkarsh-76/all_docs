KNN : K nearest neighbours
NN  : Neural Network
LM  : Linear Model
NT  : Non Tree
EDA : Exploratory Data Analysis

Step-1
  Preprocessing
    Numeric Features
      1> Feature scaling- mainly for KNN, NN, linear models(sklearn.preprocessing.MinMaxScalar/StandardScalar)
      2> Outliers - Remove outliers manually or clip data vetween two percentiles(eg 1 and 99){callef winsorization}
      3> (Optional) Rank Transformation - data to ranke(useful to bring outliers closer)(LM, KNN, NN)(scipy.stats.rankdata)
      4> (Optional) Log transormation/Raise to the power(sqrt,square, etc.)(NT sp. NN)
      5> (Optional) Concat data generated by different preprocessing/mix model based on differently processed data(LM, KNN, NN)
      
    Categorical and Ordinal Features
      1> Label Encoding - switching categorical feature to numeric(sklearn.preprocessing.LabelEncoder/pandas.factorize/
                          frequency encoding)
      2> One-Hot Encoding - Convert 1 categorical feature to multiple columns with 0 and 1 at appropriate places
                            (pandas.get_dummies/sklearn.preprocessing.OneHotEncoder)
                            
    Text
      1> Lowercase      - Convert everything to lowercase
      2> Lemmatization  - carefully converting words which are basically the same(e.g. democratic, democratization, 
                          democracy to democracy)
      3> Stemming       - Converting words which are basically the same(e.g. democratic, democratization, democracy to democr)
      4> Stopwords      - remove common occuring words/Articles/prepositions(eg. of,is,are etc.)
                        - max_df feature of CountVectorizer
      
      
    
Step-2
  Feature Generation
    Numeric Features
      1> Prior Knowledge
      2> EDA
    
    Categorical and Ordinal Features
      1> Feature interaction b/w categorical variables - Combining 2 or more categorical features to generate new features
                                                         (LM, KNN, NN)
    
    Datetime
      1> Time period - time inside particular event(campaign time)
      2> Time Since - Time passed since a particular event(days since last holiday)
      3> Difference between dates - (days diff b/w last purchase date and last customer complain date)
      
    Coordinates
      1> e.g. - use coordinates of some landmark to calculate its distance from data
      
      
      
Step-3
  Handlin Missing Values -- Hidden NaNs - (-1) having a peak, lot of data point having same value
    1> Fill with perticular value(-1/-99)
    2> Fill with Mean/Mode/Median
    3> Reconstruct values
    4> Generate new feature isnull
    5> Xgboost can handle NaNs by itself
    
    
    
Step-4
  Feature extraction
    From Text
      1> Bag of Words - count frequency of each word and build a table(sklearn.feature_extraction.text.CountVectorizer)
                      - need to preprocess this to scale features(term frequency tranformation) and boost imp. 
                        ones(inverse documentFrequency)
                      - sklearn.feature_extraction.text.TfidfVectorizer(TFiDf transformation)
                      - N-grams -> use n consecutive n words instead of single word(use Ngram_range, 
                        analyzer(char) in CountVectorizer)
                        
      2> Word2vec     - Convert word to vector(similarly used words are close in this multi-dimensionl space)
                      - Can do sum/difference on these vectors

    From Images
      vector from Images
      1> Descriptors
      2> Train Network from scratch
      3> Finetuning
