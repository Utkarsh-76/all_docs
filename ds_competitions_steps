KNN : K nearest neighbours
NN  : Neural Network
LM  : Linear Model
NT  : Non Tree
EDA : Exploratory Data Analysis

Step-1
  Preprocessing
    Numeric Features
      1> Feature scaling- mainly for KNN, NN, linear models(sklearn.preprocessing.MinMaxScalar/StandardScalar)
      2> Outliers - Remove outliers manually or clip data vetween two percentiles(eg 1 and 99){callef winsorization}
      3> (Optional) Rank Transformation - data to ranke(useful to bring outliers closer)(LM, KNN, NN)(scipy.stats.rankdata)
      4> (Optional) Log transormation/Raise to the power(sqrt,square, etc.)(NT sp. NN)
      5> (Optional) Concat data generated by different preprocessing/mix model based on differently processed data(LM, KNN, NN)
      
    Categorical and Ordinal Features
      1> Label Encoding - switching categorical feature to numeric(sklearn.preprocessing.LabelEncoder/pandas.factorize/
                          frequency encoding)
      2> One-Hot Encoding - Convert 1 categorical feature to multiple columns with 0 and 1 at appropriate places
                            (pandas.get_dummies/sklearn.preprocessing.OneHotEncoder)
      3> Mean Encoding - give values as mean of target value for each class
                         ME regularization
                            - CV loop inside train data - make folds and use all except 1 fold to data to mean encode left out fold
                            - Smoothing
                            - Adding random noice - Add random noice(not very good method)
                            - Sorting and calculatin expanding mean - sord data and calculate nth category using mean of target of n-1
    
                            
    Text
      1> Lowercase      - Convert everything to lowercase
      2> Lemmatization  - carefully converting words which are basically the same(e.g. democratic, democratization, 
                          democracy to democracy)
      3> Stemming       - Converting words which are basically the same(e.g. democratic, democratization, democracy to democr)
      4> Stopwords      - remove common occuring words/Articles/prepositions(eg. of,is,are etc.)
                        - max_df feature of CountVectorizer
      
      
    
Step-2
  Feature Generation
    Numeric Features
      1> Prior Knowledge
      2> EDA   --  https://seaborn.pydata.org/
        - Building domain knowledge
        - Checking errors(Errors can be used to generate feature)
        - Understanding how the data was generated
        - Data Visualization
           >> Explore individual features
              # Histograms : number of value in each bin(plt.hist)
                             Cannot tell if a value is repeated several time or are values different in 1 bin
              # Plot index v/s values : plt.plot(x,'.')
              
           >> Explore feature relation
              # Relation b/w 2 values : plt.scatter(x1, x2)
                                        Find distinct combination of data

              # Relattion each pair of data : plt.scatter_matrix(df)(to plot all sctter plots)
                                              df.corr(), plt.matshow
                                              
              # Generate new featur using groups : e.g. df.mean().sort_values().plot(style='.')
                                              
      1> Feature interaction b/w categorical variables - Combining 2 or more categorical features to generate new features
                                                         (LM, KNN, NN)
    
    Datetime
      1> Time period - time inside particular event(campaign time)
      2> Time Since - Time passed since a particular event(days since last holiday)
      3> Difference between dates - (days diff b/w last purchase date and last customer complain date)
      
    Coordinates
      1> e.g. - use coordinates of some landmark to calculate its distance from data
      
      
      
Step-3
  Handlin Missing Values -- Hidden NaNs - (-1) having a peak, lot of data point having same value
    1> Fill with perticular value(-1/-99)
    2> Fill with Mean/Mode/Median
    3> Reconstruct values
    4> Generate new feature isnull
    5> Xgboost can handle NaNs by itself
    
    
    
Step-4
  Feature extraction
    From Text
      1> Bag of Words - count frequency of each word and build a table(sklearn.feature_extraction.text.CountVectorizer)
                      - need to preprocess this to scale features(term frequency tranformation) and boost imp. 
                        ones(inverse documentFrequency)
                      - sklearn.feature_extraction.text.TfidfVectorizer(TFiDf transformation)
                      - N-grams -> use n consecutive n words instead of single word(use Ngram_range, 
                        analyzer(char) in CountVectorizer)
                        
      2> Word2vec     - Convert word to vector(similarly used words are close in this multi-dimensionl space)
                      - Can do sum/difference on these vectors

    From Images
      vector from Images
      1> Descriptors
      2> Train Network from scratch
      3> Finetuning



Step-5
  Data Cleaning
    1> Remove constant data
    2> Drop duplicate columns(same levels can have different tags)
    3> Remove duplicate row from train and test
    4> Check that the data is shuffled
    
    
Step-6
  Validation
    Divide data we have full info about, in 2 parts(train and validation)
    1> Holdout        - Train test split
    2> K-fold         - Train test split k-times with different test
    3> Leave-one-out  - special case of k-fold with k = number of samples(specially used when we have few sample data)
    4> Time based split - Split data using time based train-test split- mainly useful when we have to predict future outputs
    
  Types of Split
    1> Random/rowwise - Randomly choose data(data can be independent, if not, exploit dependency)
    2> Timewise - Time based split (moving window split, validation data is changed by one week) 
    3> By id - split data based on id
    4> Combined - combibation of 2 and 3


Step-7
  Find Data Leakages
    1> Meta Data
    2> info in IDs
    3> Row Order
    
    
Step-8
  Metric Optimization
    1> Run the right model                                  - MSW,logloss
    2> Preprocess train and optimize another metric         - MSPE,MAPE,RMSLE
    3> Optimize another metric and postprocess predictions  - Accuracy, kappa
    4> Write custom loss function
    5> Optimize another metric, use early stop

    Scoring metrics
      Regression
        1> MSE,RMSE - (root)mean squared error
        2> MAE      - mean absolute error
        3> MSPE     - mean squared percentage error
        4> MAPE     - ---- absolute ---------
        5> RMSLE    - Root mean squared log error(Transform target:z=log(y+1), fit model with MSE loss and trasform 
                                                  predictions back:y=exp(z) - 1)

      Classification
        1> Accuracy
        2> Logarithmic loss
        3> AUC - Area under Receiver Operating Curve
        4> Cohen's Kappa`
        

Step-9
  Hyperparameter Tuning
    https://www.coursera.org/learn/competitive-data-science/lecture/wzi5a/hyperparameter-tuning-ii
    For XGboost, LightGBM and Random forest trees
